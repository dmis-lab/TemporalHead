{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer, SVDInterpreter\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM\n",
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import set_seed\n",
    "### Random Seed ###\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(SEED)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device=\"cuda:0\", \n",
    "    fold_ln=True, \n",
    "    center_writing_weights=True, \n",
    "    center_unembed=True, \n",
    "    refactor_factored_attn_matrices=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_attention_head_output(attn_out, hook, head_idx_to_ablate=None):\n",
    "    \"\"\"\n",
    "    attn_out.shape == [batch_size, seq_len, n_heads, d_head]\n",
    "    (May vary depending on model configuration)\n",
    "    head_idx_to_ablate: index of the attention head to ablate (set to 0)\n",
    "    \"\"\"\n",
    "    if head_idx_to_ablate is not None:\n",
    "        attn_out[..., head_idx_to_ablate, :] = 0.0\n",
    "    return attn_out\n",
    "\n",
    "def ablate_and_infer(model, prompt, layer_idx, head_idx, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    - Registers an ablation hook on the specified layer_idx and head_idx of the model\n",
    "    - Generates text from the given prompt with the ablated state\n",
    "    - Returns the generated text as a string\n",
    "    \"\"\"\n",
    "    # (1) Register hook\n",
    "    hook_name = f\"blocks.{layer_idx}.attn.hook_result\"\n",
    "    \n",
    "    # If any hooks have been previously registered, it is safer to reset them first\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "    \n",
    "    model.cfg.use_split_qkv_input = True\n",
    "    model.cfg.use_attn_result = True\n",
    "\n",
    "    model.add_perma_hook(\n",
    "        hook_name,\n",
    "        lambda attn_out, hook: ablate_attention_head_output(\n",
    "            attn_out,\n",
    "            hook,\n",
    "            head_idx_to_ablate=head_idx\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # (2) Tokenize prompt and generate output using the generate method\n",
    "    print(f\"[INFO] Ablation on layer {layer_idx}, head {head_idx} - Prompt: {prompt}\")\n",
    "    input_tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    generated_tokens = model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.0,   # Adjust parameters as desired\n",
    "        top_p=1\n",
    "    )\n",
    "    generated_text = model.to_string(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'In 1999, the name of president of South Korea was'\n",
    "# prompt = 'In 2004, the name of president of South Korea was'\n",
    "# prompt = 'In 2009, the name of president of South Korea was'\n",
    "\n",
    "# Model output without any ablation\n",
    "# Reset hooks to return the model to its original state\n",
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
    "\n",
    "# Generate text\n",
    "no_ablation_tokens = model.generate(\n",
    "    input_tokens,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "# Convert tokens to string\n",
    "no_ablation_answer = model.to_string(no_ablation_tokens)\n",
    "\n",
    "print(\"=== Without Ablation ===\")\n",
    "print(no_ablation_answer)\n",
    "print(\"============================================\")\n",
    "\n",
    "# (B) Example: Ablation at layer=2, head=2 (a2.h2)\n",
    "ablated_answer = ablate_and_infer(model, prompt, layer_idx=2, head_idx=2)\n",
    "print(\"=== With Ablation (layer2, head2) ===\")\n",
    "print(ablated_answer)\n",
    "print(\"============================================\")\n",
    "\n",
    "# (C) Temporal Head Ablation (layer=18, head=3 -> a18.h3)\n",
    "ablated_answer_2 = ablate_and_infer(model, prompt, layer_idx=18, head_idx=3)\n",
    "print(\"=== With Ablation (layer18, head3) ===\")\n",
    "print(ablated_answer_2)\n",
    "print(\"============================================\")\n",
    "\n",
    "# (D) Temporal Head Ablation (layer=15, head=0 -> a15.h0)\n",
    "ablated_answer_3 = ablate_and_infer(model, prompt, layer_idx=15, head_idx=0)\n",
    "print(\"=== With Ablation (layer15, head0) ===\")\n",
    "print(ablated_answer_3)\n",
    "print(\"============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ablate multiple attention heads\n",
    "def ablate_multi_attention_head_output(attn_out, hook, heads_to_ablate=None):\n",
    "    \"\"\"\n",
    "    Ablates specified attention heads by setting their outputs to zero.\n",
    "\n",
    "    Args:\n",
    "        attn_out (torch.Tensor): The attention output tensor of shape [batch_size, seq_len, n_heads, d_head].\n",
    "        hook (HookPoint): The hook point in the model.\n",
    "        heads_to_ablate (List[int], optional): List of head indices to ablate. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Modified attention output with specified heads ablated.\n",
    "    \"\"\"\n",
    "    if heads_to_ablate is not None:\n",
    "        attn_out[..., heads_to_ablate, :] = 0.0\n",
    "    return attn_out\n",
    "\n",
    "# Function to register ablation hooks for multiple heads across specified layers\n",
    "def ablate_multiple_heads(model, layer_heads_dict):\n",
    "    \"\"\"\n",
    "    Registers permanent hooks to ablate specified attention heads in given layers.\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The transformer model.\n",
    "        layer_heads_dict (Dict[int, List[int]]): Dictionary mapping layer indices to lists of head indices to ablate.\n",
    "            Example: {2: [2, 5], 18: [3, 7]} ablates heads 2 and 5 in layer 2, and heads 3 and 7 in layer 18.\n",
    "    \"\"\"\n",
    "    # Reset existing hooks to prevent conflicts\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "\n",
    "    # Register hooks for specified layers and heads\n",
    "    for layer_idx, heads_list in layer_heads_dict.items():\n",
    "        hook_name = f\"blocks.{layer_idx}.attn.hook_result\"\n",
    "        model.add_perma_hook(\n",
    "            hook_name,\n",
    "            lambda attn_out, hook, heads_list=heads_list: ablate_multi_attention_head_output(\n",
    "                attn_out,\n",
    "                hook,\n",
    "                heads_to_ablate=heads_list\n",
    "            )\n",
    "        )\n",
    "    print(f\"[INFO] Registered multi-head ablation hooks for layers and heads: {layer_heads_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary specifying which heads to ablate in which layers\n",
    "ablation_dict = {\n",
    "    # 2: [2],\n",
    "    18: [3],\n",
    "    15: [0],\n",
    "    # 1: [15],\n",
    "    # 16: [10],\n",
    "    # 20: [17],\n",
    "    # 3: [19]\n",
    "    # 18: [15],\n",
    "    # 23: [26],\n",
    "    # 10: [13],\n",
    "    # 12: [6],\n",
    "    # 3: [4],\n",
    "    # 17: [15]\n",
    "}\n",
    "\n",
    "# Register the ablation hooks\n",
    "ablate_multiple_heads(model, ablation_dict)\n",
    "\n",
    "# Inference with the model after ablation\n",
    "prompt = 'In 1999, the name of president of South Korea was'\n",
    "# prompt = 'In 2004, the name of president of South Korea was'\n",
    "# prompt = 'In 2009, the name of president of South Korea was'\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "\n",
    "# Generate text with the ablated model\n",
    "ablated_tokens = model.generate(\n",
    "    input_tokens,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "# Convert generated tokens to string\n",
    "ablated_answer = model.to_string(ablated_tokens)\n",
    "\n",
    "print(\"=== With Multi-Head Ablation ===\")\n",
    "print(ablated_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
