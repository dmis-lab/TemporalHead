{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    Qwen2ForCausalLM,\n",
    "    Phi3ForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    (model_name := \"meta-llama/Llama-2-7b-chat-hf\"),\n",
    "    # (model_name := \"Qwen/Qwen1.5-7B-Chat\"),\n",
    "    # (model_name := \"microsoft/Phi-3-mini-4k-instruct\"),\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "model = model.eval().requires_grad_(False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Llama-2-7b-chat-hf ######\n",
    "# TEMPORAL_HEAD_MASKING = \"none\"\n",
    "# TEMPORAL_HEAD_MASKING = \"a18h3\"\n",
    "# TEMPORAL_HEAD_MASKING = \"a15h0\"\n",
    "TEMPORAL_HEAD_MASKING = \"a18h3,a15h0\"\n",
    "\n",
    "######## Qwen1.5-7B-Chat #######\n",
    "# TEMPORAL_HEAD_MASKING = \"none\"\n",
    "# TEMPORAL_HEAD_MASKING = \"a17h15\"\n",
    "\n",
    "#### Phi-3-mini-4k-instruct ####\n",
    "# TEMPORAL_HEAD_MASKING = \"none\"\n",
    "# TEMPORAL_HEAD_MASKING = \"a10h13\"\n",
    "\n",
    "if TEMPORAL_HEAD_MASKING != \"none\":\n",
    "    for masking in TEMPORAL_HEAD_MASKING.split(\",\"):\n",
    "        layer, head = map(int, re.match(r\"a(\\d+)h(\\d+)\", masking).groups())\n",
    "        if isinstance(\n",
    "            model,\n",
    "            (LlamaForCausalLM, Qwen2ForCausalLM, Phi3ForCausalLM),\n",
    "        ):\n",
    "            print(layer, head)\n",
    "            o_proj = model.model.layers[layer].self_attn.o_proj.weight.data\n",
    "            o_proj = o_proj.unflatten(-1, (model.config.num_attention_heads, -1))\n",
    "            o_proj[:, head, :] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"./data/Temporal/*.json\"):\n",
    "    with open(filename) as fp:\n",
    "        dataset = json.load(fp)\n",
    "\n",
    "    candidates = {x[\"subject\"]: x[\"objects\"] for x in dataset[\"all_possible_objects\"]}\n",
    "    candidates_len = {\n",
    "        k: torch.as_tensor(list(map(len, v))) for k, v in candidates.items()\n",
    "    }\n",
    "    dataset[\"predictions\"] = {}\n",
    "\n",
    "    for key in (\"prompt_templates\", \"prompt_templates_zs\"):\n",
    "        for i, prompt in enumerate(dataset[key]):\n",
    "            dataset[\"predictions\"][f\"{key}:{i}\"] = []\n",
    "            for sample in tqdm.tqdm(dataset[\"samples\"], desc=f\"{key}:{i}\"):\n",
    "                if key.endswith(\"_zs\"):\n",
    "                    texts = [\n",
    "                        tokenizer.apply_chat_template(\n",
    "                            [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt.format(\n",
    "                                        time=sample[\"time\"], subject=sample[\"subject\"]\n",
    "                                    ),\n",
    "                                },\n",
    "                                {\"role\": \"assistant\", \"content\": \"The answer is: \" + x},\n",
    "                            ],\n",
    "                            tokenize=False,\n",
    "                        )\n",
    "                        for x in candidates[sample[\"subject\"]]\n",
    "                    ]\n",
    "                    texts.append(\n",
    "                        tokenizer.apply_chat_template(\n",
    "                            [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt.format(\n",
    "                                        time=sample[\"time\"], subject=sample[\"subject\"]\n",
    "                                    ),\n",
    "                                },\n",
    "                            ],\n",
    "                            tokenize=False,\n",
    "                            add_generation_prompt=True,\n",
    "                        )\n",
    "                    )\n",
    "                    # print(texts[0])\n",
    "                    # print(texts[-1])\n",
    "                else:\n",
    "                    texts = [\n",
    "                        prompt.format(time=sample[\"time\"], subject=sample[\"subject\"])\n",
    "                        + \" \"\n",
    "                        + x\n",
    "                        for x in candidates[sample[\"subject\"]]\n",
    "                    ]\n",
    "                    texts.append(\n",
    "                        prompt.format(time=sample[\"time\"], subject=sample[\"subject\"])\n",
    "                    )\n",
    "\n",
    "                encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "                encodings = encodings.to(\"cuda\")\n",
    "                # print(encodings.input_ids.shape)\n",
    "                logits = model(**encodings).logits\n",
    "\n",
    "                loss = nn.functional.cross_entropy(\n",
    "                    logits[:, :-1].permute(0, 2, 1).float(),\n",
    "                    encodings.input_ids[:, 1:],\n",
    "                    reduction=\"none\",\n",
    "                )\n",
    "                logprobs = -(loss * encodings.attention_mask[:, 1:]).sum(-1)\n",
    "                logprobs = logprobs[:-1] - logprobs[-1]\n",
    "                logprobs_norm = logprobs.cpu() / candidates_len[sample[\"subject\"]]\n",
    "\n",
    "                probs = logprobs.softmax(0).tolist()\n",
    "                prob_norms = logprobs_norm.softmax(0).tolist()\n",
    "\n",
    "                dataset[\"predictions\"][f\"{key}:{i}\"].append(\n",
    "                    {\n",
    "                        **sample,\n",
    "                        # \"logprobs\": dict(zip(candidates, logprobs.tolist())),\n",
    "                        # \"logprobs_norm\": dict(zip(candidates, logprobs_norm.tolist())),\n",
    "                        \"probs\": dict(zip(candidates[sample[\"subject\"]], probs)),\n",
    "                        # \"probs_norm\": dict(\n",
    "                        #     zip(candidates[sample[\"subject\"]], prob_norms)\n",
    "                        # ),\n",
    "                    }\n",
    "                )\n",
    "                del logits, loss, logprobs, logprobs_norm, probs, prob_norms\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    filename = filename.replace(\n",
    "        \"./data/\",\n",
    "        f\"./logprob/outputs/{os.path.basename(model_name)}/{TEMPORAL_HEAD_MASKING}/\",\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"w\") as fp:\n",
    "        json.dump(dataset, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(\"./data/Invariant/*.json\"):\n",
    "    with open(filename) as fp:\n",
    "        dataset = json.load(fp)\n",
    "\n",
    "    candidates = sorted(set([x[\"object\"] for x in dataset[\"samples\"]]))\n",
    "    candidates_len = torch.as_tensor(list(map(len, candidates)))\n",
    "    dataset[\"predictions\"] = {}\n",
    "\n",
    "    for key in (\"prompt_templates\", \"prompt_templates_zs\"):\n",
    "        for i, prompt in enumerate(dataset[key]):\n",
    "            dataset[\"predictions\"][f\"{key}:{i}\"] = []\n",
    "            for sample in tqdm.tqdm(dataset[\"samples\"], desc=f\"{key}:{i}\"):\n",
    "                if key.endswith(\"_zs\"):\n",
    "                    texts = [\n",
    "                        tokenizer.apply_chat_template(\n",
    "                            [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt.format(subject=sample[\"subject\"]),\n",
    "                                },\n",
    "                                {\"role\": \"assistant\", \"content\": \"The answer is: \" + x},\n",
    "                            ],\n",
    "                            tokenize=False,\n",
    "                        )\n",
    "                        for x in candidates\n",
    "                    ]\n",
    "                    texts.append(\n",
    "                        tokenizer.apply_chat_template(\n",
    "                            [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": prompt.format(subject=sample[\"subject\"]),\n",
    "                                },\n",
    "                            ],\n",
    "                            tokenize=False,\n",
    "                            add_generation_prompt=True,\n",
    "                        )\n",
    "                    )\n",
    "                    # print(texts[0])\n",
    "                    # print(texts[-1])\n",
    "                else:\n",
    "                    texts = [\n",
    "                        prompt.format(subject=sample[\"subject\"]) + \" \" + x\n",
    "                        for x in candidates\n",
    "                    ]\n",
    "                    texts.append(prompt.format(subject=sample[\"subject\"]))\n",
    "\n",
    "                encodings = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
    "                encodings = encodings.to(\"cuda\")\n",
    "                logits = model(**encodings).logits\n",
    "\n",
    "                loss = nn.functional.cross_entropy(\n",
    "                    logits[:, :-1].permute(0, 2, 1).float(),\n",
    "                    encodings.input_ids[:, 1:],\n",
    "                    reduction=\"none\",\n",
    "                )\n",
    "                logprobs = -(loss * encodings.attention_mask[:, 1:]).sum(-1)\n",
    "                logprobs = logprobs[:-1] - logprobs[-1]\n",
    "                logprobs_norm = logprobs.cpu() / candidates_len\n",
    "\n",
    "                probs = logprobs.softmax(0).tolist()\n",
    "                prob_norms = logprobs_norm.softmax(0).tolist()\n",
    "\n",
    "                dataset[\"predictions\"][f\"{key}:{i}\"].append(\n",
    "                    {\n",
    "                        **sample,\n",
    "                        # \"logprobs\": dict(zip(candidates, logprobs.tolist())),\n",
    "                        # \"logprobs_norm\": dict(zip(candidates, logprobs_norm.tolist())),\n",
    "                        \"probs\": dict(zip(candidates, probs)),\n",
    "                        # \"probs_norm\": dict(zip(candidates, prob_norms)),\n",
    "                    }\n",
    "                )\n",
    "                del logits, loss, logprobs, logprobs_norm, probs, prob_norms\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    filename = filename.replace(\n",
    "        \"./data/\",\n",
    "        f\"./logprob/outputs/{os.path.basename(model_name)}/{TEMPORAL_HEAD_MASKING}/\",\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"w\") as fp:\n",
    "        json.dump(dataset, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for filename in glob.glob(\"./logprob/outputs/**/*.json\", recursive=True):\n",
    "    _, model, head, temporal, task = filename.split(\"/\")\n",
    "    with open(filename) as fp:\n",
    "        preds = json.load(fp)\n",
    "    preds = preds[\"predictions\"][\"prompt_templates_zs:0\"]\n",
    "    score = np.mean(\n",
    "        [\n",
    "            x[\"probs\"][\"Philip Condit\" if x[\"object\"] == \"Phil Condit\" else x[\"object\"]]\n",
    "            for x in preds\n",
    "        ]\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"head\": head,\n",
    "            \"temporal\": temporal,\n",
    "            \"task\": task,\n",
    "            \"score\": score,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_excel(\"./logprob/results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
